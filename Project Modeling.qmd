---
title: "Project Modeling"
format: html
editor: visual
---

## Introduction

In this file, we will be continuing the analysis of the diabetes data set from the EDA file. This file will focus on modeling the data using a training data set to fit a classification tree and random forest model. The use of these models will allow us to predict the Diabetes_binary variable. We will finally narrow the selection of models to only our best classification tree and our best random forest model using log-loss with five-fold cross validation, before comparing these final two to declare an overall best model. 

## Data Preparation

Before we can start modeling, we need to split the data into training (70%) and test (30%) sets.
```{r}
library(tidymodels)
set.seed(123)

split <- initial_split(diabetes, prop = 0.7, strata = Diabetes_binary)
train_diabetes <- training(split)
test_diabetes <- testing(split)
```

## Classification Tree

Next, we will fit a classification tree model to the data using varying values for the parameter and using five-fold CV on the training set to evaluate the best model. We will use the five predictors described in our EDA performed in the previous file. For clarity, a classification tree model splits the feature space into disjoint regions to create a tree of decisions, with each node having a predicted class distribution. The class with the highest probability is the predicted label.
```{r}
#First we will need a recipe to set up our 5 predictors
rec <- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth, data = train_diabetes)

#Our metric will be log-loss
logloss <- metric_set(mn_log_loss)

#Creating our five folds for evaluation
cv_folds <- vfold_cv(train_diabetes, v = 5, strata = Diabetes_binary)

#We are now ready to make the classification tree model
tree_spec <- decision_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) |>
  set_engine("rpart")

tree_wf <- workflow() |>
  add_model(tree_spec) |>
  add_recipe(rec)

#Tuning grid for the tree
tree_grid <- grid_random(
  cost_complexity(range = c(-4, -2), trans = exp_trans()),
  tree_depth(range = c(2L, 8L)),
  min_n(range=c(10L, 40L)),
  size = 12
)

#Tune
set.seed(567)

tree_tune <- tune_grid(
  tree_wf,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = logloss,
  control = control_grid(verbose = TRUE)
)
```

Now lets pick the best classification tree model.
```{r}
best_tree <- select_best(tree_tune, metric = "mn_log_loss")
final_tree_wf <- finalize_workflow(tree_wf, best_tree)
```

## Random Forest

A random forest model is an ensemble model of many decision trees whose predictions are aggregated, by majority vote for classification trees and by average for regression trees. Advantages of using a random forest model include much less variance of predictors when compared to a single tree and better predictive performance often when compared to single trees. 
```{r}
set.seed(123)

#recipe for the same predictors as before
rec_rf <- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth, data = train_diabetes)

# model specifications with tuning
rf_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  min_n = 10,
  trees = 200
) |>
  set_engine("ranger")

rf_wf <- workflow() |>
  add_model(rf_spec) |>
  add_recipe(rec_rf)

#Our metric will be log-loss
logloss <- metric_set(mn_log_loss)

#Creating our five-fold cross validation for this model
cv_folds <- vfold_cv(train_diabetes, v = 5, strata = Diabetes_binary)

#Tuning grid
rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  levels = 5
)

#Tune
rf_tune <- tune_grid(
  rf_wf,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = logloss,
  control = control_grid()
)
```

Now let us pick the best model out of the random forest models generated.
```{r}
best_rf <- select_best(rf_tune, metric = "mn_log_loss")
final_rf_wf <- finalize_workflow(rf_wf, best_rf)
```

## Final Model Selection

Now that we have one best model for both classification tree and random forest model, we can fit both to the test data and evaluate the effectiveness of each model in order to declare an overall winner.
```{r}
#First we evaluate each model on the test set
tree_test_results <- last_fit(
  final_tree_wf,
  split,
  metrics = metric_set(mn_log_loss)
)

rf_test_results <- last_fit(
  final_rf_wf,
  split,
  metrics = metric_set(mn_log_loss)
) 

#Then we use log-loss metrics to declare a winner
tree_logloss <- collect_metrics(tree_test_results)
rf_logloss <- collect_metrics(rf_test_results)
tree_logloss
rf_logloss
```

Based on our evaluations, it appears that the random forest model has the lowest log-loss output although by a slim margin. Therefore we have declared it the best model.

Lets save some important model details for later use in our api file
```{r}
saveRDS(final_rf_wf, "final_rf_wf.rds")
```
